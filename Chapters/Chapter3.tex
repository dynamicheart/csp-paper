% Chapter Template

\chapter{存储分离} % Main chapter title

\label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
\section{背景}
%-----------------------------------
%	SUBSECTION 1
%-----------------------------------
\subsection{存储技术的发展}
存储技术的起源很早，发展迅猛，存储介质的先进性与存储性价比都在不断飞升。
1725年，Basile Bouchon发明了打孔纸卡，可以打数百个孔记录二值数据，不仅数据量小，而且很不便利，不能复用，
极低的性价比使得没有人真正用他来存储数据。
1946年RCA公司开始对计数电子管展开研究，发明了长为10英寸，能存储4KB数据的计数电子管，
开始转向电子数据存储。
1950年，IBM将盘式磁带用在数据存储上，成为80年代前最为普及的计算机存储设备。
1956年，IBM公司发明了第一块硬盘，并在短时间内不断优化，奠定了硬盘发展方向。
1989年，第一款固态硬盘成功被研发，其优越的存储速度远超机械硬盘。


纵观存储技术的发展史，始终符合大容量，高速度，可复用的理念以及逐渐提高的性价比。
但是，在可持续发展观念深入人心的今日，资源利用率一直是热门话题。
如何提高存储资源的利用率成为了存储技术发展的新挑战。
%-----------------------------------
%	SUBSECTION 2
%-----------------------------------
\subsection{存储技术新挑战}
大多数主流的存储设备结构采用的是直连式存储，即存储设备与服务器主机一体化，
只有唯一的服务器主机能访问该存储设备。
考虑如下应用场景，有两台一一对应的主机与1T容量存储设备，其中一台主机A执行数据库服务，
需要1.5T的数据存储空间；另一台主机B执行web后台服务，对存储空间需求极小。
由于每台服务器主机只能使用属于自己的1T存储空间，显然A无法顺利执行。
从另一方面看，B则闲置了大量存储资源，出现了资源分配不均等导致资源浪费和资源紧缺问题。
事实上，总计2T的存储空间，完全可以满足A,B二机的需求，只是缺乏了合理的资源分配制度。
%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------
\section{现有解决方案}
%-----------------------------------
%	SUBSECTION 1
%-----------------------------------
\subsection{传统方案}
%-----------------------------------
%	SUBSUBSECTION 1
%-----------------------------------
\subsubsection{直连式存储}
直连式存储(Direct-attached storage, DAS)是将外置存储设备通过小型计算机系统接口(Small computer system interface, SCSI)
直接连接到服务器主机上的资源存储方式，其特征是服务器主机与存储资源一体化。
物理连接也是最为简单直接的连接方式，实现便捷，不需要专业人员的维护，能节省维护成本。
但服务器主机的SCSI ID资源很有限，能够与之通过DAS方式连接的存储设备是有限的，所以每台服务器主机所能管理的存储资源有上限，存储资源扩展性很差。
（图~\ref{fig:das_architecture}）为DAS架构图。

\begin{figure}
\centering
\includegraphics[scale=0.45]{Figures/storage/das_architecture.jpg}
\decoRule
\caption{DAS架构}
\label{fig:das_architecture}
\end{figure}

%-----------------------------------
%	SUBSUBSECTION 2
%-----------------------------------
\subsubsection{存储区域网络}

存储区域网络(Storage Area Network, SAN)是通过网状通道(Fibre Channel, FC)交换机连接外置存储设备，建立专用于数据存储的区域网络。
在SAN模式下，服务器主机通过TCP/IP协议与存储设备通信。相比于DAS模式，SAN模式的基础是一个专用网络，可以自由地在SAN系统中添加存储设备，可扩展性大大加强。
（图~\ref{fig:san_architecture}）为SAN架构图。

\begin{figure}
\centering
\includegraphics[scale=0.45]{Figures/storage/san_architecture.jpg}
\decoRule
\caption{SAN架构}
\label{fig:san_architecture}
\end{figure}

%-----------------------------------
%	SUBSUBSECTION 3
%-----------------------------------
\subsubsection{网络附属存储}

网络附属存储(Network Attached Storage, NAS)是采用网络技术(TCP/IP, ATM, FDDI等)通过网络交换机连接存储系统和服务器主机来建立存储私网的存储模式。
NAS模式与SAN模式最大的区别在于NAS模式的文件系统位于存储设备端，其网络带宽消耗较大。
（图~\ref{fig:nas_architecture}）为NAS架构图。

\begin{figure}
\centering
\includegraphics[scale=0.45]{Figures/storage/nas_architecture.jpg}
\decoRule
\caption{NAS架构}
\label{fig:nas_architecture}
\end{figure}
%-----------------------------------
%	SUBSECTION 2
%-----------------------------------
\subsection{存储资源分离技术}
%-----------------------------------
%	SUBSUBSECTION 1
%-----------------------------------
\subsubsection{Amazon EBS}
Amazon Elastic Block Store(EBS)\cite{amazon2010amazon}块级别的储存功能，这些块直接与Amazon EC2
实例连接，保证了数据的有效性和安全性。
Amazon EBS依据存储性能和工作负载成本为指标，提供了两类方案选择：
一是用于事务性工作负载的SSD备份存储，例如数据库等，
二是用于吞吐量密集型工作负载的HDD备份存储，例如MapReduce和日志处理等。
Amazon EBS技术已经大量应用于Amazon的关系型数据库服务中，提供了良好的效益。
%-----------------------------------
%	SUBSUBSECTION 2
%-----------------------------------
\subsubsection{VMware Virtual SAN}
Virtual SAN\cite{vmware2013vmware}是VMware针对存储设备解耦提出的资源存储技术。Virtual SAN
与vSphere相互融合，聚合了虚拟化管理程序的体系结构，
将存储设备池化为共享数据存储，提供更高的数据共享性。
同时，读写缓存功能一定程度上缓解了SAN带来的性能损失，提供最短的IO路径。

\begin{figure}
\centering
\includegraphics[scale=0.8]{Figures/storage/virtualSAN_architecture.jpg}
\decoRule
\caption{Virtual SAN架构}
\label{fig:virtualSAN_architecture}
\end{figure}
%-----------------------------------
%	SUBSUBSECTION 3
%-----------------------------------
\subsubsection{Petal:Distributed Virtual Disks}
Petal\cite{lee1996petal}的存储架构应用了分布式的虚拟硬盘。各个服务器主机可自由地应用不同的文件系统，
通过可扩展的网络层对读写需求进行转发，下发至各个存储硬盘服务器。
该架构对服务器主机的文件系统不做限制，并且有很好的容错率和单点错误恢复能力。
\begin{figure}
\centering
\includegraphics[scale=0.8]{Figures/storage/dvd_architecture.jpg}
\decoRule
\caption{Petal分布式存储架构}
\label{fig:dvd_architecture}
\end{figure}
%-----------------------------------
%	SUBSUBSECTION 4
%-----------------------------------
\subsubsection{Parallax:Managing Storage for a Million Machines}
Parallax\cite{warfield2005parallax}提出VM架构以管理百万数量机群的存储管理，凭借灵活轻量的快照机制和简单的分布式块存储来实现高效复制并提高可用性。
但其存储机制必然引入额外性能开销，实现上对之进行权衡。

\begin{figure}
\centering
\includegraphics[scale=0.8]{Figures/storage/parallax.jpg}
\decoRule
\caption{Parallax VM架构}
\label{fig:parallax}
\end{figure}
%-----------------------------------
%	SUBSUBSECTION 5
%-----------------------------------
\subsubsection{Blizzard:Block Storage}
Blizzard\cite{mickens2014blizzard}块存储架构中，虚拟设备通过维护VID记录实际存储设备的位置，从而将得到的读写指令转发到相应的物理设备上。
对读写数据的合理缓存机制能够适当减小读写开销。同时日志机制使得系统的安全性和可靠性大幅上升，也增加了系统的容错机制。
文章中对Blizzard块存储架构与Amazon的EBS架构进行量化对比，证明Blizzard的SQLIOPS性能远比EBS高。

\begin{figure}
\centering
\includegraphics[scale=0.8]{Figures/storage/blizzard.jpg}
\decoRule
\caption{Blizzard存储架构}
\label{fig:blizzard}
\end{figure}
%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------

\section{案例分析：Flash Storage Disaggregation}

%-----------------------------------
%	SUBSECTION 1
%-----------------------------------
\subsection{架构设计}

图~\ref{fig:d-a_flash_architecture}展示了传统的每台主机拥有一块闪存设备的架构，这意味着应用只能访问到对应的直连的闪存设备。
图~\ref{fig:dis_flash_architecture}是分离式闪存架构，其特点是闪存层(Flash Tier)从数据存储层(DataStore Tier)中解耦出来，
使得主机能够通过网络小型计算机接口(internet small computer system interface)访问所有的闪存设备，
并在闪存层增加协调管理器(coordination manager)，负责选择合理的存储资源，以达到提高闪存资源利用率的目的。

\begin{figure}
\centering
\includegraphics[scale=0.8]{Figures/storage/d-a_flash_architecture.jpg}
\decoRule
\caption{直连式闪存架构}
\label{fig:d-a_flash_architecture}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.8]{Figures/storage/dis_flash_architecture.jpg}
\decoRule
\caption{分离式闪存架构}
\label{fig:dis_flash_architecture}
\end{figure}

%-----------------------------------
%	SUBSECTION 2
%-----------------------------------
\subsection{读写流程}
%-----------------------------------
%	SUBSUBSECTION 1
%-----------------------------------
\subsubsection{写流程}
iSCSI层的initiator能控制传输数据的时间，数据存储层维护一个指向协议数据单元(Protocol Data Unit, PDU)的指针。
iSCSI层对要写的数据进行包装（加入信息头等），然后由内核将PDU传输给闪存层。
其中包括的NIC和SCSI的缓存读写则取决与TCP/IP栈的实现。
%-----------------------------------
%	SUBSUBSECTION 2
%-----------------------------------
\subsubsection{读流程}
iSCSI的initiator接收到来自闪存层的包后，将使用DMA(Direct Memory Access)技术将包的数据从NIC模块转移至内核内存。
内核解析获取的包，拆离出数据内容，将之放在iSCSI的PDU。iSCSI层将PDU数据复制到SCSI缓存。
此时应用就可以从SCSI缓存中直接获取数据。

%-----------------------------------
%	SUBSECTION 3
%-----------------------------------
\subsection{性能评估}
%-----------------------------------
%	SUBSUBSECTION 1
%-----------------------------------
\subsubsection{延迟分析}
数据存储层使用RocksDB键值存储，利用mutilate负载生成器\cite{leverich2014mutilate}生成大量用户进程，进行大量的查询操作，
营造期望的每秒查询率(Queries Per Second, QPS)。图~\ref{fig:read_latency_CDF}展示了在无负载系统下，
本地闪存与远程闪存读取数据的用户延迟的累积分布函数(Cumulative Distribution Function)。
远程闪存的用户延迟显然高于本地闪存，在曲线的95\%处，远程闪存读取延迟约比本地闪存高260微秒。
由于延迟SLA在5到10毫秒左右，远远高于微秒数量级，所以远程闪存的延迟代价在可接受范围内。

\begin{figure}
\centering
\includegraphics[scale=0.8]{Figures/storage/read_latency_CDF.jpg}
\decoRule
\caption{读操作延迟的CDF曲线}
\label{fig:read_latency_CDF}
\end{figure}

%-----------------------------------
%	SUBSUBSECTION 2
%-----------------------------------
\subsubsection{吞吐量分析}
为测试本地闪存和远程闪存的吞吐量指标，控制应用层的每秒请求率(QPS)，并描绘点（QPS，延迟），图~\ref{fig:QPS_latency}展示其吞吐量性能。
由图分析，在相同延迟情况下，远程闪存吞吐量约为本地闪存的80\%。表观上，远程闪存的吞吐量受到了较严重的影响。
一方面，这是存储设备性能和资源利用率等的权衡，远程闪存的理念是牺牲了部分性能，换取更好的资源利用率。
另一方面，可以通过扩展数据存储层的CPU来补偿这些吞吐量损失。

\begin{figure}
\centering
\includegraphics[scale=0.8]{Figures/storage/QPS_latency.jpg}
\decoRule
\caption{单SSDB服务器的延迟-QPS曲线}
\label{fig:QPS_latency}
\end{figure}

%-----------------------------------
%	SUBSUBSECTION 3
%-----------------------------------
\subsubsection{敏感度分析}
通过在SSDB中增加循环进程，创造不同CPU负载的环境，测试远程闪存的QPS对不同参数的曲线变化率（敏感度）。
图~\ref{fig:QPS_CPUintensity}展示了本地闪存和远程闪存在不同CPU负载下的QPS。二者在CPU负载较低的环境下，本地闪存较远程闪存有更高的QPS，
意味着远程闪存的性能受到一定影响，而在CPU处于较高负载的环境下，本地闪存与远程闪存的QPS相差无几，
意味着此时主要瓶颈不在iSCSI通讯上。同时，本地闪存的QPS受CPU负载大小的影响比远程闪存大，也就是说，远程闪存的性能表现更加稳定。


图~\ref{fig:QPS_percentage}展示了在相同请求数量下，本地闪存和远程闪存的QPS随写请求占总请求的比例变化曲线。
二者的QPS都随着写请求比例增加而增加，因为写操作的返回是异步的，其效率比读操作高。
二者的QPS差值在不同的写操作比例下差别不大，可以得出结论，读写操作所占比例对本地闪存和远程闪存性能差距影响不大。

\begin{figure}
\centering
\includegraphics[scale=0.8]{Figures/storage/QPS_CPUintensity.jpg}
\decoRule
\caption{QPS随CPU负载变化曲线}
\label{fig:QPS_CPUintensity}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.8]{Figures/storage/QPS_percentage.jpg}
\decoRule
\caption{QPS随读操作占比变化曲线}
\label{fig:QPS_percentage}
\end{figure}

%-----------------------------------
%	SUBSUBSECTION 4
%-----------------------------------
\subsubsection{多服务端性能分析}
实际情况中，机群中服务器主机与闪存时多对多的关系，所以分析多服务端共享同闪存集群的性能很有实际价值。
图~\ref{fig:2SSDB_latency_QPS}和图~\ref{fig:3SSDB_latency_QPS}分别展示两个SSDB服务器主机和三个SSDB服务器主机时，本地闪存与远程闪存的延迟-QPS曲线图。
毫无疑问，本地闪存的QPS性能始终比远程闪存更高。值得一提的是，三个SSDB环境相较于两个SSDB环境下，远程闪存的QPS性能损失代价更严重。

\begin{figure}
\centering
\includegraphics[scale=0.8]{Figures/storage/2SSDB_latency_QPS.jpg}
\decoRule
\caption{2个SSDB服务共享闪存下，延迟-QPS曲线}
\label{fig:2SSDB_latency_QPS}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.8]{Figures/storage/3SSDB_latency_QPS.jpg}
\decoRule
\caption{3个SSDB服务共享闪存下，延迟-QPS曲线}
\label{fig:3SSDB_latency_QPS}
\end{figure}

%-----------------------------------
%	SUBSECTION 4
%-----------------------------------
\subsection{存储分离适用情境分析}
通过计算直连式闪存(Direct-attached flash)和远程闪存(disaggregation flash)的主要消耗C$_{direct}$和C$_{disagg}$，对存储分离进行优势评估。
直连式闪存的消耗包括闪存读写的消耗和数据存储端CPU、RAM、NIC的消耗两部分，远程闪存则包括闪存读写的消耗，数据存储层CPU、RAM、NIC的消耗和闪存层CPU、RAM、NIC的消耗。
这些消耗的权重为目标数据与实际数据的比值，如目标QPS与实际QPS的比值。
计算而得的消耗越大，意味着其资源利用越差，反之则越好。
引入新的评估指标，消耗节约比例。
消耗节约比例含义为使用远程闪存比直连式闪存节约的消耗比例。
若消耗节约比例值为正，则表示该环境参数下远程闪存消耗比直连式闪存小；反之则远程闪存消耗更大。


图~\ref{fig:compute_storage}展示了在计算强度比例因子和存储空间比例因子分布空间下，相应的消耗节约比例。
由图例的对称性可以分析而得，闪存读写的消耗与数据存储层的CPU、RAM、NIC消耗大致相等。
在不同的情景假设下（对应图中不同的二维坐标），对资源的分配权衡也是不一样的。
若闪存资源的价值比CPU、内存低，那么服务器将倾向高闪存空间和低计算强度，对应图例的右下半边区域，该区域的消耗节约比例大于零，
意味着远程闪存消耗比直连式闪存低，则此情境下，服务器主机更适合于远程闪存。
若闪存资源的价值比CPU、内存高，那么服务器将倾向低闪存空间和高计算强度，对应图例的坐上半边区域，同理，该区域的消耗节约比例大于零，
远程闪存消耗比直连式闪存低，服务器主机更适合于远程闪存。
若闪存资源与CPU、内存资源价值相当，对应图例的对角线区域，该区域的消耗节约比例小于零，
也就是远程闪存消耗比直连式闪存高，此时服务器主机更适合直连式闪存。

\begin{figure}
\centering
\includegraphics[scale=0.7]{Figures/storage/compute_storage.jpg}
\decoRule
\caption{消耗节约比例-计算强度比例因子-存储空间比例因子分布图}
\label{fig:compute_storage}
\end{figure}

%-----------------------------------
%	SUBSECTION 5
%-----------------------------------
\subsection{本章小结}
Klimovic等人\cite{klimovic2016flash}对闪存分离策略做了充分的研究，分析了闪存分离带来的容量和资源利用率优势，
也客观展示了资源分离带来的性能损失，并提出了弥补性能损失的方法。
在考虑服务器主机整体架构时，不能盲目地强调闪存分离的优势，忽略其带来的性能损失和维护成本。
更加客观的策略是，通过各类资源（如CPU,内存，闪存）的当前价值来定量计算分离式闪存带来的提升及损失孰重孰轻，进而合理取舍。

%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------

\section{总结}
本节我们探讨了存储资源分离的背景和发展历程，简介了Amazon的EBS架构，VMware的SAN架构等，描绘了存储技术发展的现状。
并以Klimovic等人\cite{klimovic2016flash}的闪存分离研究工作为切入点，分析了闪存分离的优势及不足，对其作出客观评价。
总结来说，存储资源分离能够实现高程度的存储设备解耦，使得资源管理更加高效便捷，能够大幅提高存储资源利用率，一定程度上避免存储资源浪费。
但存储资源分离使得存储框架复杂化，带来额外开发维护成本，且利用网络作为数据传输介质，无疑使得性能也有相当的损耗。
目前大多数解决方案关注点在于架构的设计，力求存储资源分配更加合理化，实际上，减小数据传输带来的性能损耗也是一大研究方向。
利用良好的数据缓存等机制，从根本上减少通过网络传输数据的频率，或许能成为降低性能损耗的良策。















